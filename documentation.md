```markdown
# T"AI" - Technical Documentation

This document provides an in-depth technical overview of the T"AI" (Your Personalized AI Teaching Assistant) system. It is aimed at software engineers and developers who need a detailed understanding of the codebase, architecture, and workflows.

---

## Overview

T"AI" is a Streamlit-based educational platform that integrates with various AI models and services. It provides functionalities for:

- **User Management (Professors & Students)**: Authentication, role-based access control.
- **Course Management**: Creating, updating, and deleting courses.
- **Document Ingestion & Processing**: Uploading PDFs and text files, embedding, and vectorizing documents.
- **AI-Powered Features**:
  - **Question Answering**: Retrieval-augmented Q&A over uploaded course materials.
  - **Summarization**: High-level summaries of course content.
  - **MCQ & Flashcard Generation**: Automated question and flashcard creation to aid study.
  - **Podcast Generation**: Converting documents into audio form using text-to-speech (gTTS).
- **YouTube Integration**: Transcription and semantic retrieval of relevant YouTube content.
- **Analytics & Insights**: Topic classification, CSV-based logging, and data visualization.

---

## Architecture

**Front-End**:  
- **Streamlit** for UI/UX: Responsive front-end to interact with the system.
- **Session State**: Manages user sessions and state across page interactions.

**Back-End**:  
- **Database & ORM**: SQLite DB via SQLAlchemy for persistent storage of users, courses, and files.
- **Data Models**:
  - **User**: Stores `username`, `password_hash`, `role`.
  - **Course**: Linked to a `User` (professor) and can store references to `CourseFile` and `YouTube` links.
  - **CourseFile**: Binary data for course materials (PDFs, transcripts).
  - **StudentQuestion**: Logs every student question.

- **AI Integration**:
  - **LangChain**: Abstracts LLM interactions, document loaders, chunking, retrieval, and prompt templates.
  - **OpenAI API**: GPT-4 or GPT-3.5 models for text generation, embeddings.
  - **FAISS**: Vector store for semantic retrieval of documents.
  - **SentenceTransformers**: For additional embedding and semantic similarity tasks.
  - **gTTS**: Converts generated podcast scripts into MP3 audio files.
  - **AssemblyAI** (optional): For YouTube transcript extraction.

**Data Flow**:
1. **Document Upload** → **Extraction & Chunking** → **Embedding** → **FAISS Vector Store**.
2. **User Query** → **Vector Retrieval** → **LLM Prompt** → **LLM Response** (grounded in docs).
3. **Analytics**: User questions are logged, topics inferred, and CSV updated.

---

## File Structure

```plaintext
project/
├─ application.py         # Main Streamlit application (UI, routes, logic)
├─ application1.py        # Additional UI components, CSS injection, static pages
├─ base.py                # Base declarative for SQLAlchemy models
├─ models.py              # SQLAlchemy models (User, Course, CourseFile, StudentQuestion)
├─ database.py            # Database session factory (SessionLocal)
├─ data/                  # CSV files for analytics (e.g., ml_grouped_topics_questions.csv)
├─ img/                   # Image assets for UI styling
├─ requirements.txt        # Python dependencies
└─ ... (other resources)
```

---

## Key Components

### `application.py`

**Core logic file** that:

- **User Pages**:  
  - `signup_page()`: Registers new users, stores hashed passwords.
  - `login_page()`: Authenticates users using bcrypt-hashed passwords.
  
- **Professor Dashboard**:
  - `professor_page()`: Provides UI for creating courses, uploading materials, adding YouTube links.
  - Document ingestion pipeline:
    - PDF/text files → Extracted text via `PyPDF2`.
    - Documents chunked by `CharacterTextSplitter` (LangChain).
    - Embeddings generated by `OpenAIEmbeddings`.
    - Indexed by a FAISS Vector Store.

- **Student Dashboard**:
  - `student_page()`: Lists courses, allows Q&A with course content.
  - Q&A:
    - User question → Vector retrieval for top-k docs.
    - LangChain `RetrievalQA` chain with GPT → returns contextually relevant answers.
  - MCQs & Flashcards:
    - LLMChain with prompts to generate questions and flashcards from doc text.
  - Summarization:
    - `load_summarize_chain` (LangChain) to summarize documents using `map_reduce`.

- **Podcast Feature**:
  - Extract text from selected PDFs.
  - Prompt GPT to create a podcast script.
  - Use `gTTS` to convert script → MP3, served to user.

- **YouTube Integration**:
  - Validate URLs.
  - Download audio with `yt-dlp`.
  - Transcribe audio via AssemblyAI.
  - Integrate transcript into vector store.

- **Insights & Analytics**:
  - Logs student questions to CSV.
  - Topic classification via keyword matching.
  - Visualizations with Plotly (Pie/Bar charts) and WordCloud.
  - LLM-generated CSV report.

### `application1.py`

- Defines UI styling and helper functions for:
  - About page
  - Contact page
  - Home page UI
- Injects CSS, handles image overlays.

### `models.py` and `base.py`

- Defines the ORM schemas for `User`, `Course`, `CourseFile`, `StudentQuestion`.
- Relationships between tables:
  - `User` ↔ `Course` (One-to-Many)
  - `Course` ↔ `CourseFile` (One-to-Many)
  - `Course` ↔ `StudentQuestion` (One-to-Many)
  - `User` ↔ `StudentQuestion` (One-to-Many)

### `database.py`

- Configures `SessionLocal` for SQLAlchemy.
- Ensures that database sessions are properly instantiated and managed.

---

## External Services and APIs

- **OpenAI**: Requires `OPENAI_API_KEY`.
- **AssemblyAI**: Requires `ASSEMBLYAI_API_KEY` for audio transcription.
- **YouTube**: Uses `yt-dlp` command-line tool for downloading audio, parsing results.

---

## Setup & Configuration

- **Environment Variables**:
  - `OPENAI_API_KEY`: Your OpenAI key.
  - `ASSEMBLYAI_API_KEY`: Your AssemblyAI key (optional).
- **Virtual Environment**:
  ```bash
  pip install -r requirements.txt
  ```

- **Running the App**:
  ```bash
  streamlit run application.py
  ```
  
- Access at `http://localhost:8501`.

---

## Technical Considerations

- **Chunking Strategy**:  
  Documents are chunked into 2,000-character segments with a 100-character overlap to ensure semantic continuity. Tune `chunk_size` and `chunk_overlap` as needed.

- **Retrieval Method**:  
  FAISS vector stores are used. Future improvements may involve switching to a managed vector DB (e.g., Pinecone) or updating embeddings to newer models.

- **Prompt Templates**:
  - Summarization, MCQ, Flashcard generation relies on custom `PromptTemplate` instances.
  - Consider adjusting temperature and model choices for different tasks.

- **Caching & Performance**:
  - Currently uses `InMemoryCache`.
  - Scale by implementing persistent caching or a vector database.
  - Proper batching of embeddings could speed up large document processing.

- **Security & Access Control**:
  - Simple password hashing with bcrypt.
  - No JWT or OAuth yet. Could be integrated for production use.
  - Role checks ensure that only professors can create or manage courses.

---

## Analytics & Insights

- **CSV Logging**:
  - Each student question is appended to a CSV: `ml_grouped_topics_questions.csv`.
  - Can be used for offline analysis or feeding back into the model for improvement.

- **Visualization**:
  - Uses Plotly for pie and bar charts.
  - Generates word clouds from student questions.
  - LLM-based report summarizing CSV insights.

---

## Error Handling & Logging

- **Logging**:
  - Standard Python `logging` at `INFO` and `ERROR` levels.
  - Critical paths (document load, LLM calls) wrapped in try/except with logged errors.

- **Error Messages**:
  - Streamlit displays user-friendly messages for known issues (empty uploads, no API keys, invalid YouTube links).

---

## Extensibility

- **Additional Document Types**:
  - Implement loaders for DOCX, HTML.
- **Additional LLMs**:
  - Swap `ChatOpenAI` with other providers (Anthropic, Azure OpenAI).
- **Scaling**:
  - Integrate a cloud-based database.
  - Containerize the app with Docker.
  - Deploy on Streamlit Cloud or other hosting services.

---

## Conclusion

This documentation outlines the technical aspects and workflows of the T"AI" system. By following the architecture, understanding the code structure, and utilizing the provided prompts and pipelines, developers can maintain, extend, and optimize this AI-driven educational assistant.
```
